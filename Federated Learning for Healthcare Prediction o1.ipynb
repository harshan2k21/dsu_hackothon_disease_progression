{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db2d01f-ca26-4730-933c-bdec8015456c",
   "metadata": {},
   "source": [
    "# 1. Import Libraries\n",
    "First, you need to import the required Python libraries. This is typically done in the first cell of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277c5d2d-e038-439d-b8b1-0a666bd67adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9e1f0-17d7-42e7-86dd-b9242a4c9fe0",
   "metadata": {},
   "source": [
    "# 2. Helper Function for Debugging\n",
    "Add the debug print function to help trace issues during execution. You may choose to toggle these prints on/off during different stages of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a38c64-3e79-432e-b854-8864fef62386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Debugging Utility\n",
    "def debug_print(message):\n",
    "    print(f\"DEBUG: {message}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fe390-30cc-4673-bb8d-33c4145021dc",
   "metadata": {},
   "source": [
    "# 3. Define the Neural Network Model\n",
    "Next, define the architecture of the neural network HealthcarePredictionModel. This cell can be modified later to tune the model structure if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ef66ce-8e09-45f9-b518-0615788fb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Neural Network Model\n",
    "class HealthcarePredictionModel(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(HealthcarePredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(torch.relu(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn3(torch.relu(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681a2f7-2fd7-46c5-bd4d-c48ca53bb909",
   "metadata": {},
   "source": [
    "# 4. Data Loading and Preprocessing\n",
    "This cell defines a function to load the CSV data and preprocess it, including handling missing values, converting categorical \n",
    "variables, and scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01feb2ee-fabb-4a3a-9eb8-8fa6e1106dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    try:\n",
    "        debug_print(f\"Loading data from {file_path}\")\n",
    "        file_path = file_path.strip('\"')\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        debug_print(\"Handling missing values\")\n",
    "        data = data.dropna()\n",
    "        \n",
    "        debug_print(\"Converting categorical variables\")\n",
    "        categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "        data = pd.get_dummies(data, columns=categorical_columns)\n",
    "        \n",
    "        debug_print(\"Separating features and labels\")\n",
    "        features = data.iloc[:, :-1].values\n",
    "        labels = data.iloc[:, -1].values\n",
    "        \n",
    "        debug_print(\"Normalizing features\")\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "        \n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {file_path}: {str(e)}\")\n",
    "        print(\"Traceback:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac64bd9-bc90-451a-9d86-64161d2d22f0",
   "metadata": {},
   "source": [
    "# 5. Federated Training Logic\n",
    "The federated training logic involves training local models on hospital datasets and aggregating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8734e6b2-7cd9-432f-8a92-a8d071ffa227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Federated Training Logic\n",
    "def train_federated(model, hospital_loaders, val_loader, epochs=50, lr=0.0005):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        debug_print(f\"Starting epoch {epoch + 1}\")\n",
    "        worker_models = []\n",
    "        epoch_loss = 0\n",
    "        for i, loader in enumerate(hospital_loaders):\n",
    "            debug_print(f\"Training on hospital dataset {i + 1}\")\n",
    "            local_model = HealthcarePredictionModel(input_features=next(iter(loader))[0].shape[1])\n",
    "            local_model.load_state_dict(model.state_dict())\n",
    "            local_optimizer = optim.Adam(local_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            local_scheduler = optim.lr_scheduler.ReduceLROnPlateau(local_optimizer, 'min', patience=5, factor=0.5)\n",
    "            \n",
    "            local_model.train()\n",
    "            batch_losses = []\n",
    "            for data, target in loader:\n",
    "                local_optimizer.zero_grad()\n",
    "                output = local_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n",
    "                local_optimizer.step()\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            avg_loss = np.mean(batch_losses)\n",
    "            local_scheduler.step(avg_loss)\n",
    "            epoch_loss += avg_loss\n",
    "            worker_models.append(local_model.state_dict())\n",
    "        \n",
    "        debug_print(\"Aggregating models\")\n",
    "        avg_model = {}\n",
    "        for key in model.state_dict().keys():\n",
    "            avg_model[key] = torch.mean(torch.stack([worker_model[key].float() for worker_model in worker_models]), dim=0)\n",
    "        model.load_state_dict(avg_model)\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(hospital_loaders))\n",
    "        \n",
    "        debug_print(\"Validating model\")\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                val_preds.extend(pred.cpu().numpy())\n",
    "                val_true.extend(target.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3187c-3698-4995-8889-1c971ddb5242",
   "metadata": {},
   "source": [
    "# 6. Main Execution\n",
    "1. The main execution logic where you:\n",
    "2. Load hospital datasets.\n",
    "3. Initialize the model.\n",
    "4. Train the model using federated learning.\n",
    "5. Plot results and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0caa0a-f43d-4187-95e4-a24ec37b1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the file path for hospital 1 dataset (CSV format):  \"C:\\Users\\Harshan\\Documents\\dsu hackothon\\hospital_1_1data.csv\"\n",
      "Enter the file path for hospital 2 dataset (CSV format):  \"C:\\Users\\Harshan\\Documents\\dsu hackothon\\hospital_1_1data.csv\"\n",
      "Enter the file path for hospital 3 dataset (CSV format):  \"C:\\Users\\Harshan\\Documents\\dsu hackothon\\hospital_1_1data.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Loading data from C:\\Users\\Harshan\\Documents\\dsu hackothon\\hospital_1_1data.csv\n",
      "DEBUG: Handling missing values\n",
      "DEBUG: Converting categorical variables\n",
      "DEBUG: Separating features and labels\n",
      "DEBUG: Normalizing features\n",
      "DEBUG: Loading data from C:\\Users\\Harshan\\Documents\\dsu hackothon\\hospital_1_1data.csv\n",
      "DEBUG: Handling missing values\n",
      "DEBUG: Converting categorical variables\n",
      "DEBUG: Separating features and labels\n",
      "DEBUG: Normalizing features\n",
      "DEBUG: Loading data from C:\\Users\\Harshan\\Documents\\dsu hackothon\\hospital_1_1data.csv\n",
      "DEBUG: Handling missing values\n",
      "DEBUG: Converting categorical variables\n",
      "DEBUG: Separating features and labels\n",
      "DEBUG: Normalizing features\n",
      "DEBUG: Creating validation set\n",
      "DEBUG: Creating DataLoaders\n",
      "DEBUG: Initializing and training the model\n",
      "DEBUG: Starting epoch 1\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 1, Loss: 0.0701, Val Accuracy: 0.9946\n",
      "DEBUG: Starting epoch 2\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 2, Loss: 0.0268, Val Accuracy: 0.9968\n",
      "DEBUG: Starting epoch 3\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 3, Loss: 0.0216, Val Accuracy: 0.9971\n",
      "DEBUG: Starting epoch 4\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 4, Loss: 0.0195, Val Accuracy: 0.9977\n",
      "DEBUG: Starting epoch 5\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 5, Loss: 0.0184, Val Accuracy: 0.9970\n",
      "DEBUG: Starting epoch 6\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 6, Loss: 0.0176, Val Accuracy: 0.9979\n",
      "DEBUG: Starting epoch 7\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 7, Loss: 0.0169, Val Accuracy: 0.9982\n",
      "DEBUG: Starting epoch 8\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n",
      "DEBUG: Training on hospital dataset 3\n",
      "DEBUG: Aggregating models\n",
      "DEBUG: Validating model\n",
      "Epoch 8, Loss: 0.0163, Val Accuracy: 0.9979\n",
      "DEBUG: Starting epoch 9\n",
      "DEBUG: Training on hospital dataset 1\n",
      "DEBUG: Training on hospital dataset 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Execution\n",
    "# Get user input for file paths\n",
    "hospital_files = []\n",
    "for i in range(3):\n",
    "    while True:\n",
    "        file_path = input(f\"Enter the file path for hospital {i + 1} dataset (CSV format): \")\n",
    "        file_path = file_path.strip('\"')\n",
    "        if os.path.exists(file_path):\n",
    "            hospital_files.append(file_path)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            print(\"Please enter a valid file path.\")\n",
    "\n",
    "# Load and prepare data\n",
    "hospital_datasets = []\n",
    "for file in hospital_files:\n",
    "    data, target = load_and_preprocess_data(file)\n",
    "    if data is not None and target is not None:\n",
    "        dataset = TensorDataset(data, target)\n",
    "        hospital_datasets.append(dataset)\n",
    "\n",
    "if not hospital_datasets:\n",
    "    print(\"No valid data loaded. Exiting program.\")\n",
    "    exit()\n",
    "\n",
    "debug_print(\"Creating validation set\")\n",
    "combined_dataset = torch.utils.data.ConcatDataset(hospital_datasets)\n",
    "train_size = int(0.8 * len(combined_dataset))\n",
    "val_size = len(combined_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "debug_print(\"Creating DataLoaders\")\n",
    "hospital_loaders = [DataLoader(dataset, batch_size=64, shuffle=True) for dataset in hospital_datasets]\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "debug_print(\"Initializing and training the model\")\n",
    "input_features = next(iter(hospital_loaders[0]))[0].shape[1]\n",
    "global_model = HealthcarePredictionModel(input_features=input_features)\n",
    "train_losses, val_accuracies = train_federated(global_model, hospital_loaders, val_loader, epochs=50, lr=0.0005)\n",
    "\n",
    "debug_print(\"Plotting training progress\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b8e61-1dc3-404d-b9b2-6c8b355b0194",
   "metadata": {},
   "source": [
    "# 7. Test Set Evaluation\n",
    "Finally, handle user input for the test set and evaluate the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265fcf5-1121-4425-a55e-8471d57573b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test Set Evaluation\n",
    "while True:\n",
    "    test_file_path = input(\"Enter the file path for the test dataset (CSV format): \")\n",
    "    test_file_path = test_file_path.strip('\"')\n",
    "    if os.path.exists(test_file_path):\n",
    "        break\n",
    "    else:\n",
    "        print(f\"File not found: {test_file_path}\")\n",
    "        print(\"Please enter a valid file path.\")\n",
    "\n",
    "debug_print(\"Evaluating on test set\")\n",
    "test_data, test_labels = load_and_preprocess_data(test_file_path)\n",
    "global_model.eval()\n",
    "with torch.no_grad():\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4719fb-cd5a-4768-8206-605b26aa7345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafbf61-470a-4e54-bfc1-1792c73bb1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537dce30-9411-4536-8ec5-26a3b910457e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7e39b-a64a-486b-9e89-c28801f027ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ec149-4800-443d-a8f0-f266ed6030b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc823e9-43f4-4c07-9970-7ed32128d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df4f50-c291-4760-9d86-a4a763a6bb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ed28d-40fc-443c-a786-4d357c5ed06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008246f6-eb86-4872-9bb3-845a87c992c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c1934-a073-4bd7-b8c5-a90f34ee7024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e164dee-b6bc-401f-a183-5d4319987943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
